{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model V1\n",
    "\n",
    "**Summary**: Changed model to be implemented in PyTorch (doesn't include LSTM). Uses a definitely working implementation of the Policy Gradient algorithm. Environment observation returns differences in prices rather than entire prices.\n",
    "\n",
    "-------------------------\n",
    "## The Model\n",
    "Before, the model encountered problems where it was constantly buying in the tensorflow model. To reduce the risk of an error on my part, I have re-written a simpler model in PyTorch. PyTorch uses dynamic construction of the computation graph so it will have better support later on when implementing a LSTM. The current shape of the model is as follows: Input (5) -> Layer 1 (64) -> Layer 2 (32) -> Layer 3 (64) -> Layer 4 (16) -> Layer 5 (3). As a result of these changes, I expect the accuracy of the model to get above a 65% for the current implementation of the archeticture to be labeled as a success. The model has also been given the function `generate_action_call()` which allows for it to create the call objects to the environment. The `amount` variable will later be the subject to changing as it controls how much stock the agent wishes to buy or sell.\n",
    "\n",
    "**TODO**\n",
    "* Determine How Much To Buy\n",
    "-----------------------------\n",
    "## The Environment\n",
    "Only a couple of changes have been made to the environment. They include ...\n",
    "* Holding_Rate changed to 0.1 -> 0.01\n",
    "* State holds records of the current days changes in prices. The returned value of today displayed to the model is not the raw number, but rather ** today - yesterday ** for all 5 features (Volume, Open, High, Low, Close)\n",
    "\n",
    "**Posible Changes** in later iterations maybe be to include ...\n",
    "* Include Algo Trading Ratios (Sharpe Ratio...) into the input into the model\n",
    "------------------------------------------\n",
    "## Training\n",
    "The optimization occurs after every episode because the rewards and depreciation would be messed up if it occured after every episode. The entire loop was re-written with PyTorch in mind. Also, the agent will not always be reseted to the 0th day of the dataset because not reseting of the dataset occurs with exception to the `env.load_stock()` function.\n",
    "**Hyperparameters Check**\n",
    "* `n_epochs = 500` |  How many stocks will be iterated throught\n",
    "* `n_batches = 10`  |  How many runs on a single stock in a row\n",
    "* `learning_rate = 1e-4`  |  Amount of change to take when making changes to the model's weights\n",
    "* `max_steps_per_episode = 90`  |  Maximum amount of days the agent can run before being terminated\n",
    "* `running_reward = 10` |  Initial reward value for reward averager (Depreciated Overtime)\n",
    "* `reward_threshold = 300`  |  Reward needed to obtain success on this model's run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
